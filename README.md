# Let's explore Spark DataFrames

Dataframe analysis in PySpark!
<!-- [(home)](https://dmerz75.github.io/spark2_dfanalysis)
[(git-home)](https://github.com/dmerz75/spark2_dfanalysis) -->
<!-- [Iridium](https://dmerz75.github.io/iridium_catalyst/) for details. -->
<!-- ## Start Spark | Construct Dataframes | Read/Write -->

Let's build some dataframes so that we have something with which to work.
We'll do some basic reading, writing, partitioning and maybe discuss certain
parameter advantages and disadvantages.

## The Basics:
  - [Initial Configuration / Spark settings for your Jupyter notebook!](./pages/construct/Standard_Configs.html)
  <!-- (pages/construct/Initial_Configuration.md) -->

  Quickly begin your fresh notebook here!

  - [Build/Construct](./pages/construct/Building_DataFrames1.html)

  In case you don't have big data sets, build some quickly!

  - [Read/Write/Partition]

  Let's get some standard reading, writing and partitioning examples down.

(coming soon!)

  - Read in dataframe, do a basic comparison.
  - Read/write for loop with some count/logical comparisons.


## Common Transformations and Analysis:
Let's do some common dataframe manipulations. I'll show what I expect are some
common column formatting issues, occurrences, and operations you're likely to see.
We'll cover aggregations, grouping, and ordering.
  - [Transformations](pages/common/Transformations.md)

  - [Analysis](pages/common/Analysis.md)

(coming soon!)

  - Joins


## In Progress:
  - [Basic Operations](notebooks\incomplete\dev_basic_ops_2dataframes.html)
  - [Build examples](notebooks\incomplete\examples_build_dataframe.html)
  - [More Build Examples](notebooks\incomplete\dev_build_dataframe.html)
  - [For Loop Write & Check](notebooks\incomplete\dev_ForLoopWriteCheck.html)
  - [Read Write Partition](notebooks\incomplete\dev_read_write_partition.html)
