# Let's explore Spark DataFrames

Dataframe analysis in PySpark!
<!-- [(home)](https://dmerz75.github.io/spark2_dfanalysis)
[(git-home)](https://github.com/dmerz75/spark2_dfanalysis) -->
<!-- [Iridium](https://dmerz75.github.io/iridium_catalyst/) for details. -->
<!-- ## Start Spark | Construct Dataframes | Read/Write -->

Let's build some dataframes so that we have something with which to work.
We'll do some basic reading, writing, partitioning and maybe discuss certain
parameter advantages and disadvantages.

### The Basics:
  - [Initial Configuration / Spark settings for your Jupyter notebook!](pages/build/Initial_Configuration.md)
  Quickly begin your fresh notebook here!
  - [Build/Construct](pages/build/Building_DataFrames.md)
  In case you don't have big data sets, build some quickly right here!
  - [Read/Write/Partition](pages/build/Read_Write_Partition.md)
  Let's get some standard reading, writing and partitioning examples down.

(coming soon!)

  - Read in dataframe, do a basic comparison.
  - Read/write for loop with some count/logical comparisons.


## Common Transformations and Analysis
Let's do some common dataframe manipulations. I'll show what I expect are some
common column formatting issues, occurrences, and operations you're likely to see.
We'll cover aggregations, grouping, and ordering.
  - [Transformations](pages/common/Transformations.md)
  - [Analysis](pages/common/Analysis.md)
  - Joins (coming soon!)
