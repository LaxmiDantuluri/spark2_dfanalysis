# Let's explore Spark DataFrames

Dataframe analysis in PySpark!
<!-- [(home)](https://dmerz75.github.io/spark2_dfanalysis)
[(git-home)](https://github.com/dmerz75/spark2_dfanalysis) -->
<!-- [Iridium](https://dmerz75.github.io/iridium_catalyst/) for details. -->
<!-- ## Start Spark | Construct Dataframes | Read/Write -->

Let's build some dataframes so that we have something with which to work.
We'll do some basic reading, writing, partitioning and maybe discuss certain
parameter advantages and disadvantages.

## [Configs:](./pages/configs/Configs.md)

  Quickly begin your fresh notebook here!

  Initial Configuration / Spark settings for your Jupyter notebook!]

## [Basics:](./pages/basics/Basics.md)

(read, write, generate a sample DF ..)

  In case you don't have big data sets, build some quickly!


  <!-- (./pages/basics/Building_DataFrames1.html) -->

  Let's get some standard reading, writing and partitioning examples down.

  - [Read/Write/Partition]

(coming soon!)

  - Read in dataframe, do a basic comparison.
  - Read/write for loop with some count/logical comparisons.


## [Analysis:](pages/analysis/Analysis.md)
Let's do some common dataframe manipulations. I'll show what I expect are some
common column formatting issues, occurrences, and operations you're likely to see.
We'll cover aggregations, grouping, and ordering.

  <!-- - [Analysis] -->

## [Transformations:](pages/transformations/Transformations.md)
  <!-- - [Transformations] -->

(coming soon!)

## [Joins:](pages/joins/Joins.md)

  - Joins

## Intermediate

(coming soon!)

## In Progress:
  - [Basic Operations](notebooks\incomplete\dev_basic_ops_2dataframes.html)
  - [Build examples](notebooks\incomplete\examples_build_dataframe.html)
  - [More Build Examples](notebooks\incomplete\dev_build_dataframe.html)
  - [For Loop Write & Check](notebooks\incomplete\dev_ForLoopWriteCheck.html)
  - [Read Write Partition](notebooks\incomplete\dev_read_write_partition.html)
