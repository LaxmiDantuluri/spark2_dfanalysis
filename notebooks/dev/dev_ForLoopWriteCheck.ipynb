{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Standard Spark Session!\n",
    "\n",
    "# Python libraries:\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from dateutil import parser\n",
    "# import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import builtins\n",
    "import json\n",
    "import functools\n",
    "import operator\n",
    "from itertools import product\n",
    "\n",
    "# Numpy & Pandas!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Spark!\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"myapp\").getOrCreate()\n",
    "\n",
    "#     spark = SparkSession.builder.master(\"yarn\")\\\n",
    "#     .config(\"spark.executor.instances\", \"32\")\\\n",
    "#     .config(\"spark.executor.cores\", \"4\")\\\n",
    "#     .config(\"spark.executor.memory\", \"4G\")\\\n",
    "#     .config(\"spark.driver.memory\", \"4G\")\\\n",
    "#     .config(\"spark.executor.memoryOverhead\",\"4G\")\\\n",
    "#     .config(\"spark.yarn.queue\",\"Medium\")\\\n",
    "#     .appName(\"myapp\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.debug.maxToStringFields\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Python Version:  3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]\n",
      "    Python Path:  C:\\Users\\d810216\\AppData\\Local\\conda\\conda\\envs\\my_root\n",
      " My Python Libs:  C:\\Users\\d810216/.myconfigs\n",
      "   My Spark Dir:  C:\\Users\\d810216/spark2_dfanalysis\n",
      "   My Spark Ctx:  <pyspark.sql.session.SparkSession object at 0x0000029909A1D828>\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# The autoreload extension is already loaded. To reload it, use:\n",
    "#  %reload_ext autoreload\n",
    "\n",
    "\n",
    "# mylib:\n",
    "my_library = os.path.expanduser('~/.myconfigs')\n",
    "my_spark = os.path.expanduser('~/spark2_dfanalysis')\n",
    "sys.path.append(my_library)\n",
    "sys.path.append(my_spark)\n",
    "\n",
    "\n",
    "from shared.app_context import *\n",
    "from builder.DataFrameBuild import *\n",
    "\n",
    "ctx = ApplicationContext(\"Dev-Job\")\n",
    "\n",
    "DFB = DataFrameBuild(ctx.spark)\n",
    "\n",
    "print(\"%16s  %s\" % (\"Python Version:\",sys.version))\n",
    "print(\"%16s  %s\" % (\"Python Path:\",os.path.dirname(sys.executable)))\n",
    "print(\"%16s  %s\" % (\"My Python Libs:\",my_library))\n",
    "print(\"%16s  %s\" % (\"My Spark Dir:\",my_spark))\n",
    "print(\"%16s  %s\" % (\"My Spark Ctx:\",ctx.spark))\n",
    "# print(ctx.spark)\n",
    "# print(os.listdir(my_spark))\n",
    "# print(sys.path)\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d810216\\spark2_dfanalysis\\example_notebooks\\dev\n"
     ]
    }
   ],
   "source": [
    "my_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "print(my_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = os.path.join(my_dir,'data_raw')\n",
    "dest   = os.path.join(my_dir,'data_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_source = spark.read.parquet(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a bunch of files, sort them:\n",
    "lst_files = glob.glob(source)\n",
    "\n",
    "# Get dates\n",
    "lst_dates = sorted([x.split(\"=\")[-1] for x in lst_files],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing!\n",
    "\n",
    "for date in lst_dates:\n",
    "    \n",
    "    print(date)\n",
    "    source = \"%s/fiscal_week=%s\" % (config['source'][cont_panel],date)\n",
    "    dest = \"%s/fiscal_week=%s\" % (config['target'][cont_panel],date)\n",
    "    #print(fn)\n",
    "    \n",
    "    # read\n",
    "    df_source = spark.read.parquet(fn)\n",
    "    \n",
    "    # transform logic\n",
    "    df_source_mod = df.withColumnRenamed(\"household_id\",\"ehhn\")\n",
    "    \n",
    "    # write\n",
    "    df1.coalesce(1).write.parquet(df_source_mod,mode=\"overwrite\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check!\n",
    "\n",
    "for date in lst_dates:\n",
    "    \n",
    "    #print(date)\n",
    "    source = \"%s/fiscal_week=%s\" % (config['source'][cont_panel],date)\n",
    "    dest = \"%s/fiscal_week=%s\" % (config['target'][cont_panel],date)\n",
    "    \n",
    "    #print(fn)\n",
    "    df_source = spark.read.parquet(source)\n",
    "    df_dest = spark.read.parquet(fn1)\n",
    "    c0 = df.count()\n",
    "    c1 = df1.count()\n",
    "    print(\"Date: %s %11d %11d %7d\" % (date,c0,c1,c0-c1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
