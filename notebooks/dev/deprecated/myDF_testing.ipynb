{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "# %autoreload 2\n",
    "# The autoreload extension is already loaded. To reload it, use:\n",
    "#  %reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\n"
     ]
    }
   ],
   "source": [
    "print(os.path.dirname(sys.executable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mylib:\n",
    "my_library = os.path.expanduser('~/.myconfigs')\n",
    "my_spark = os.path.expanduser('~/.spark2')\n",
    "sys.path.append(my_library)\n",
    "sys.path.append(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'C:\\\\Users\\\\d810216\\\\AppData\\\\Local\\\\Temp\\\\spark-73e6b3ef-32bb-4dff-8c1e-c5841f562eed\\\\userFiles-d7a851a7-d1fd-4177-8626-29790ce4e0ff', 'C:\\\\ProgramData\\\\Anaconda3\\\\python36.zip', 'C:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib', 'C:\\\\ProgramData\\\\Anaconda3', 'C:\\\\Users\\\\d810216\\\\AppData\\\\Roaming\\\\Python\\\\Python36\\\\site-packages', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\Sphinx-1.5.6-py3.6.egg', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\setuptools-27.2.0-py3.6.egg', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\d810216\\\\.ipython', 'C:\\\\Users\\\\d810216/.myconfigs', 'C:\\\\Users\\\\d810216/.spark2', 'C:\\\\Users\\\\d810216/.myconfigs', 'C:\\\\Users\\\\d810216/.spark2']\n",
      "C:\\Users\\d810216/.myconfigs\n",
      "C:\\Users\\d810216/.spark2\n",
      "['.git', '.gitignore', 'LICENSE', 'mydf', 'README.md', 'shared', '__init__.py']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)\n",
    "print(my_library)\n",
    "print(my_spark)\n",
    "print(os.listdir(my_spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('myappname').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accumulator', 'AccumulatorParam', 'BasicProfiler', 'Broadcast', 'HiveContext', 'MarshalSerializer', 'PickleSerializer', 'Profiler', 'RDD', 'Row', 'SQLContext', 'SparkConf', 'SparkContext', 'SparkFiles', 'SparkJobInfo', 'SparkStageInfo', 'StatusTracker', 'StorageLevel', 'TaskContext', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'accumulators', 'broadcast', 'cloudpickle', 'conf', 'context', 'copy_func', 'files', 'find_spark_home', 'heapq3', 'java_gateway', 'join', 'keyword_only', 'profiler', 'rdd', 'rddsampler', 'resultiterable', 'serializers', 'shuffle', 'since', 'sql', 'statcounter', 'status', 'storagelevel', 'taskcontext', 'traceback_utils', 'types', 'util', 'version', 'wraps']\n",
      "['Column', 'DataFrame', 'DataFrameNaFunctions', 'DataFrameReader', 'DataFrameStatFunctions', 'DataFrameWriter', 'GroupedData', 'HiveContext', 'Row', 'SQLContext', 'SparkSession', 'UDFRegistration', 'Window', 'WindowSpec', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'absolute_import', 'catalog', 'column', 'conf', 'context', 'dataframe', 'functions', 'group', 'readwriter', 'session', 'streaming', 'types', 'utils', 'window']\n",
      "['Aggregator', 'AutoBatchedSerializer', 'BatchedSerializer', 'BoundedFloat', 'CartesianDeserializer', 'CloudPickleSerializer', 'ExternalGroupBy', 'ExternalMerger', 'ExternalSorter', 'NamedTemporaryFile', 'NoOpSerializer', 'PIPE', 'PairDeserializer', 'Partitioner', 'PickleSerializer', 'PipelinedRDD', 'Popen', 'RDD', 'RDDRangeSampler', 'RDDSampler', 'RDDStratifiedSampler', 'ResultIterable', 'SCCallSiteSync', 'StatCounter', 'StorageLevel', 'Thread', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_load_from_socket', '_parse_memory', '_prepare_for_python_RDD', '_test', '_wrap_function', 'basestring', 'bisect', 'ceil', 'chain', 'copy', 'defaultdict', 'get_used_memory', 'heapq', 'ignore_unicode_prefix', 'isinf', 'isnan', 'log', 'operator', 'os', 'pack_long', 'portable_hash', 'pow', 'python_cogroup', 'python_full_outer_join', 'python_join', 'python_left_outer_join', 'python_right_outer_join', 'random', 're', 'reduce', 'shlex', 'socket', 'sqrt', 'sys', 'unicode', 'warnings']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pyspark))\n",
    "print(dir(pyspark.sql))\n",
    "print(dir(pyspark.rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shared.app_context import *\n",
    "\n",
    "ctx = ApplicationContext(\"Dev-Job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_get_numpy_record_dtypes', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n"
     ]
    }
   ],
   "source": [
    "print(dir(ctx.spark))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_get_numpy_record_dtypes', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000002074AE52E48>\n"
     ]
    }
   ],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000002074AE52E48>\n"
     ]
    }
   ],
   "source": [
    "print(ctx.spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'C:\\\\Users\\\\d810216\\\\AppData\\\\Local\\\\Temp\\\\spark-73e6b3ef-32bb-4dff-8c1e-c5841f562eed\\\\userFiles-d7a851a7-d1fd-4177-8626-29790ce4e0ff', 'C:\\\\ProgramData\\\\Anaconda3\\\\python36.zip', 'C:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib', 'C:\\\\ProgramData\\\\Anaconda3', 'C:\\\\Users\\\\d810216\\\\AppData\\\\Roaming\\\\Python\\\\Python36\\\\site-packages', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\Sphinx-1.5.6-py3.6.egg', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\setuptools-27.2.0-py3.6.egg', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\d810216\\\\.ipython', 'C:\\\\Users\\\\d810216/.myconfigs', 'C:\\\\Users\\\\d810216/.spark2', 'C:\\\\Users\\\\d810216/.myconfigs', 'C:\\\\Users\\\\d810216/.spark2']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)\n",
    "from mydf.DataFrameBuild import *\n",
    "# from mydf.DataFrameProfile import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mydf.DataFrameBuild.myDF'>\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'randomly_increasing_array', 'simple_array', 'spark']\n",
      "<pyspark.sql.session.SparkSession object at 0x000002074AE52E48>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Index: bigint]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = myDF(ctx.spark)\n",
    "print(type(x))\n",
    "print(dir(x))\n",
    "print(x.spark)\n",
    "x.simple_array()\n",
    "#x.df_array(ctx.spark,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Index|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "|    9|\n",
      "|   10|\n",
      "|   11|\n",
      "|   12|\n",
      "|   13|\n",
      "|   14|\n",
      "|   15|\n",
      "|   16|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.simple_array(17)\n",
    "x.count\n",
    "x.df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Index|\n",
      "+-----+\n",
      "|  5.0|\n",
      "|  6.0|\n",
      "|  7.0|\n",
      "|  8.0|\n",
      "|  9.0|\n",
      "| 10.0|\n",
      "| 11.0|\n",
      "| 12.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.simple_array(5,12)\n",
    "x.df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!!!!-2\n",
      "[0. 1. 2.]\n",
      "[0, 1, 0]\n",
      "(3, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Index: double, Random Number: double]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.randomly_increasing_array(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0]\n",
      "[0, 0, 2]\n",
      "[0, 0, 0]\n",
      "[0, 1, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 0]\n",
      "[0, 0, 2]\n",
      "[0, 1, 1]\n",
      "[0, 0, 2]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 0, 1]\n",
      "[0, 1, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 1, 0]\n",
      "[0, 0, 2]\n",
      "[0, 1, 2]\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    x.randomly_increasing_array(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
