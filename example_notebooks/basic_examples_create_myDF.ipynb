{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "# %autoreload 2\n",
    "# The autoreload extension is already loaded. To reload it, use:\n",
    "#  %reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\n"
     ]
    }
   ],
   "source": [
    "print(os.path.dirname(sys.executable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mylib:\n",
    "my_library = os.path.expanduser('~/.myconfigs')\n",
    "my_spark = os.path.expanduser('~/spark2_dfanalysis')\n",
    "sys.path.append(my_library)\n",
    "sys.path.append(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d810216/.myconfigs\n",
      "C:\\Users\\d810216/spark2_dfanalysis\n",
      "['.git', '.gitignore', 'example_notebooks', 'LICENSE', 'mydf', 'README.md', 'shared', '__init__.py']\n"
     ]
    }
   ],
   "source": [
    "# print(sys.path)\n",
    "print(my_library)\n",
    "print(my_spark)\n",
    "print(os.listdir(my_spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('myappname').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(dir(pyspark))\n",
    "# print(dir(pyspark.sql))\n",
    "# print(dir(pyspark.rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shared.app_context import *\n",
    "\n",
    "ctx = ApplicationContext(\"Dev-Job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000018A92982748>\n",
      "<pyspark.sql.session.SparkSession object at 0x0000018A92982748>\n"
     ]
    }
   ],
   "source": [
    "print(spark)\n",
    "print(ctx.spark)\n",
    "# print(dir(ctx.spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(sys.path)\n",
    "from mydf.DataFrameBuild import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mydf.DataFrameBuild.myDF'>\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'decision_array_type', 'df_multidimensional', 'df_randomly_increasing_array', 'df_simple_array', 'df_string_array', 'get_string_array', 'profile_partitions', 'profile_size', 'spark']\n",
      "<pyspark.sql.session.SparkSession object at 0x0000018A92982748>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Index: bigint]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = myDF(ctx.spark)\n",
    "print(type(x))\n",
    "print(dir(x))\n",
    "print(x.spark)\n",
    "x.df_simple_array()\n",
    "#x.df_array(ctx.spark,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Development ---\n",
    "#### x = myDF(ctx.spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Index|\n",
      "+-----+\n",
      "|  5.0|\n",
      "|  6.0|\n",
      "|  7.0|\n",
      "|  8.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.df_simple_array(5,8)\n",
    "x.count\n",
    "x.df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Index|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "|    9|\n",
      "|   10|\n",
      "|   11|\n",
      "|   12|\n",
      "|   13|\n",
      "|   14|\n",
      "|   15|\n",
      "|   16|\n",
      "|   17|\n",
      "|   18|\n",
      "|   19|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.df_simple_array(1000)\n",
    "x.df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Index: double, Random Number: double]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.df_randomly_increasing_array(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Random Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Index  Random Number\n",
       "0     0.0            0.0\n",
       "1     1.0            1.0\n",
       "2     2.0            0.0\n",
       "3     3.0            2.0\n",
       "4     4.0            1.0\n",
       "5     5.0            2.0\n",
       "6     6.0            2.0\n",
       "7     7.0            7.0\n",
       "8     8.0            5.0\n",
       "9     9.0            6.0\n",
       "10   10.0            8.0\n",
       "11   11.0            0.0\n",
       "12   12.0            6.0\n",
       "13   13.0           13.0\n",
       "14   14.0            1.0\n",
       "15   15.0            0.0\n",
       "16   16.0            9.0\n",
       "17   17.0           10.0\n",
       "18   18.0           11.0\n",
       "19   19.0            6.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    x.df_randomly_increasing_array(50)\n",
    "    \n",
    "x.df.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fruit = [\"apple\",\"orange\",\"banana\",\"grape\",\"melon\",\"pineapple\",\"kiwi\",\"lemon\",\"lime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_fruit = x.spark.createDataFrame(fruit,StringType()).toDF(\"fruit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    fruit|\n",
      "+---------+\n",
      "|    apple|\n",
      "|   orange|\n",
      "|   banana|\n",
      "|    grape|\n",
      "|    melon|\n",
      "|pineapple|\n",
      "|     kiwi|\n",
      "|    lemon|\n",
      "|     lime|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fruit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_string_fields(lst):\n",
    "    n = len(lst)\n",
    "    fields = [StructField(x,StringType(),True) for x in lst]\n",
    "    return fields\n",
    "\n",
    "fields_fruit = get_string_fields(fruit)\n",
    "# df = x.spark.createDataFrame(fruits,fields_fruit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Index|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "+-----+\n",
      "\n",
      "+---------+\n",
      "|    fruit|\n",
      "+---------+\n",
      "|    apple|\n",
      "|   orange|\n",
      "|   banana|\n",
      "|    grape|\n",
      "|    melon|\n",
      "|pineapple|\n",
      "|     kiwi|\n",
      "|    lemon|\n",
      "|     lime|\n",
      "+---------+\n",
      "\n",
      "+-----+-------------+\n",
      "|Index|Random Number|\n",
      "+-----+-------------+\n",
      "|  0.0|          0.0|\n",
      "|  1.0|          0.0|\n",
      "|  2.0|          1.0|\n",
      "|  3.0|          1.0|\n",
      "|  4.0|          4.0|\n",
      "|  5.0|          1.0|\n",
      "|  6.0|          6.0|\n",
      "|  7.0|          4.0|\n",
      "|  8.0|          7.0|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = myDF(ctx.spark)\n",
    "# print(fruit)\n",
    "# print(len(fruit))\n",
    "\n",
    "df_num_fruit = X.df_simple_array(len(fruit))\n",
    "df_random_num= X.df_randomly_increasing_array(len(fruit))\n",
    "df_fruit     = X.df_string_array(fruit,\"fruit\")\n",
    "\n",
    "df_num_fruit.show()\n",
    "df_fruit.show()\n",
    "df_random_num.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Index']\n",
      "['fruit']\n",
      "['Index', 'Random Number']\n"
     ]
    }
   ],
   "source": [
    "print(df_num_fruit.columns)\n",
    "print(df_fruit.columns)\n",
    "print(df_random_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(df_num_fruit.count())\n",
    "print(df_fruit.count())\n",
    "# df_combined = df_num_fruit.join(df_fruit)\n",
    "# df_combined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_add_column(df,dfa,colname):\n",
    "    df_new = df.withColumn(colname,dfa.column(colname))\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruit']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fruit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_num_fruit\n",
    "# df_fruit\n",
    "df_f2 = df_num_fruit.unionAll(df_fruit)\n",
    "\n",
    "# unionAll appends 2nd dataframe to the end of the 1st\n",
    "# Vertical Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    Index|\n",
      "+---------+\n",
      "|        0|\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "|        6|\n",
      "|        7|\n",
      "|        8|\n",
      "|    apple|\n",
      "|   orange|\n",
      "|   banana|\n",
      "|    grape|\n",
      "|    melon|\n",
      "|pineapple|\n",
      "|     kiwi|\n",
      "|    lemon|\n",
      "|     lime|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_com = df_num_fruit.join(df_fruit,df_num_fruit['Index'] != df_fruit[\"fruit\"],\"outer\")\n",
    "\n",
    "# horizontal join, but non inner.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|Index|    fruit|\n",
      "+-----+---------+\n",
      "| null|    apple|\n",
      "| null|   orange|\n",
      "| null|   banana|\n",
      "| null|    grape|\n",
      "| null|    melon|\n",
      "| null|pineapple|\n",
      "| null|     kiwi|\n",
      "| null|    lemon|\n",
      "| null|     lime|\n",
      "|    0|     null|\n",
      "|    1|     null|\n",
      "|    2|     null|\n",
      "|    3|     null|\n",
      "|    4|     null|\n",
      "|    5|     null|\n",
      "|    6|     null|\n",
      "|    7|     null|\n",
      "|    8|     null|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_com.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------------+\n",
      "|string|integer|            double|\n",
      "+------+-------+------------------+\n",
      "|bewxkh|   5977|0.2570108879911984|\n",
      "|urwntz|   8969| 0.709797633826858|\n",
      "|eyqxtc|   4601|0.5923364338323751|\n",
      "|ctukzh|   3710|0.3102102445180932|\n",
      "+------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from mydf.DataFrameBuild import *\n",
    "\n",
    "\n",
    "X = myDF(ctx.spark)\n",
    "df_multi = X.df_multidimensional(4,3,[('string',6),\n",
    "                                      ('integer',(1000,9999)),\n",
    "                                      ('double',(0.0,1.0))])\n",
    "df_multi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------------+\n",
      "|string|integer|            double|\n",
      "+------+-------+------------------+\n",
      "|bewxkh|   5977|0.2570108879911984|\n",
      "|urwntz|   8969| 0.709797633826858|\n",
      "|eyqxtc|   4601|0.5923364338323751|\n",
      "|ctukzh|   3710|0.3102102445180932|\n",
      "+------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_multi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example 2. Another random, multidimensional dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from mydf.DataFrameBuild import *\n",
    "\n",
    "\n",
    "X = myDF(ctx.spark)\n",
    "df_multi6 = X.df_multidimensional(500,6,[('string1',6),\n",
    "                                        ('string2',13),\n",
    "                                        ('integer',(1000,9999)),\n",
    "                                        ('double1',(0.0,1.0)),\n",
    "                                        ('double2',(0.0,10000000)),\n",
    "                                        ('integer2',(1,10000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------+-------------------+------------------+--------+\n",
      "|string1|      string2|integer|            double1|           double2|integer2|\n",
      "+-------+-------------+-------+-------------------+------------------+--------+\n",
      "| pxzcfs|hdkfxolokqsdk|   2072|0.23950891343345992|1553567.2718001436|    8297|\n",
      "| tcxivu|rlftefdzayehr|   4285| 0.6514825745809212|2142944.0838629534|    3081|\n",
      "| fspxaq|xocpqobigrogl|   1504| 0.7777005105189999|3409895.7531948937|    6953|\n",
      "| efqmlr|cryujspjjfhms|   1887|  0.673793828982649| 5825303.103361736|    5312|\n",
      "| dzboiv|kbsybfmstplsm|   1890|  0.567766842657271| 6527736.112299853|    7344|\n",
      "| vmfisn|dybxylcrgehxj|   9600| 0.8276100223188335| 6396252.213045467|    9123|\n",
      "| ysvuju|yxskxswrkqxaq|   3235| 0.7532167820374222| 3639023.154605797|    9920|\n",
      "| haephv|omjybotnrfbzs|   9875| 0.5600824063143812| 6827808.162748888|    5115|\n",
      "| njgfee|nhhelkspqgaod|   4386| 0.7008183094857229| 7734117.614634358|    2070|\n",
      "| uuldxz|yscyhdluwhldy|   5047| 0.0663062205361441|  4751590.29352051|    3379|\n",
      "| iiawlg|pccdfgpmjqiim|   1901|  0.726353169827262| 946278.4777960298|    9074|\n",
      "| kmwjvw|cqshjmgddqlyu|   4085| 0.4645157758321743| 9661638.094792048|    5877|\n",
      "| hrewup|ofpaywaqbvidn|   1951| 0.7156249871873692| 2556473.231804133|    5494|\n",
      "| lrvawp|hlexzjcquhpfi|   8414|0.35481985536382066| 5423739.297458763|    1756|\n",
      "| cplbny|fleieiwujrqnn|   7854| 0.8075990824351565| 727095.4417577058|    6593|\n",
      "| otckrp|oclidmfnvgsss|   4637| 0.7417543159589052|3610667.9764653714|    7684|\n",
      "| nomrqx|munyyrevzvmyy|   2714| 0.9225669854583219| 5613850.810733815|    6298|\n",
      "| rzztwh|guyrgnnuxpypc|   2743|0.45535162073256275| 2547872.541329276|    4967|\n",
      "| dwqrfe|puqqjmipiyeea|   8287|  0.988208323312431| 1059964.705449412|    2770|\n",
      "| diwhnt|ohuxnmytvcscd|   1839|0.06336306200981956| 8983183.740045713|    1878|\n",
      "+-------+-------------+-------+-------------------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_multi6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
